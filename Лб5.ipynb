{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ri9cNOjpMWS",
        "outputId": "3224b266-0a3b-4e0d-a48e-0da20503154c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import twitter_samples\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "\n",
        "def tokenize(text):\n",
        "    return re.findall(r'\\b[a-z]+\\b', text.lower())\n",
        "\n",
        "tweets = twitter_samples.strings()\n",
        "tokens = []\n",
        "for tweet in tweets:\n",
        "    tokens.extend(tokenize(tweet))\n",
        "\n",
        "word_freq = Counter(tokens)\n",
        "total_words = sum(word_freq.values())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"shakespeare.txt\", encoding='utf-8') as f:\n",
        "    shakespeare_text = f.read()\n",
        "\n",
        "shakespeare_words = tokenize(shakespeare_text)\n"
      ],
      "metadata": {
        "id": "QZY13X3iZXU1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_letter(word):\n",
        "    return [word[:i] + word[i+1:] for i in range(len(word))]\n",
        "\n",
        "def replace_letter(word):\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    return [word[:i] + l + word[i+1:] for i in range(len(word)) for l in letters if l != word[i]]\n",
        "\n",
        "def insert_letter(word):\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    return [word[:i] + l + word[i:] for i in range(len(word)+1) for l in letters]\n",
        "\n",
        "def transpose_letters(word):\n",
        "    return [word[:i] + word[i+1] + word[i] + word[i+2:] for i in range(len(word)-1)]\n",
        "\n",
        "def edits1(word):\n",
        "    return set(delete_letter(word) + replace_letter(word) + insert_letter(word) + transpose_letters(word))\n",
        "\n",
        "def edits2(word):\n",
        "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))\n"
      ],
      "metadata": {
        "id": "ePlX3bDwZrUK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_probability(word):\n",
        "    return word_freq[word] / total_words if word in word_freq else 1 / total_words\n"
      ],
      "metadata": {
        "id": "ixfK2QHXZuvD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def min_edit_distance(s1, s2):\n",
        "    m, n = len(s1), len(s2)\n",
        "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
        "\n",
        "    for i in range(m+1): dp[i][0] = i\n",
        "    for j in range(n+1): dp[0][j] = j\n",
        "\n",
        "    for i in range(1, m+1):\n",
        "        for j in range(1, n+1):\n",
        "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
        "            dp[i][j] = min(\n",
        "                dp[i-1][j] + 1,   # delete\n",
        "                dp[i][j-1] + 1,   # insert\n",
        "                dp[i-1][j-1] + cost  # substitute\n",
        "            )\n",
        "    return dp[m][n]\n"
      ],
      "metadata": {
        "id": "2epDtO5mZxSb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correction(word):\n",
        "    candidates = (\n",
        "        [word] if word in word_freq else\n",
        "        edits1(word) & word_freq.keys() or\n",
        "        edits2(word) & word_freq.keys() or\n",
        "        [word]\n",
        "    )\n",
        "    return max(candidates, key=word_probability)\n"
      ],
      "metadata": {
        "id": "c0B7krwjZ0Gz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def make_typos(word):\n",
        "    if len(word) < 4:\n",
        "        return word  # занадто коротке\n",
        "    return random.choice([\n",
        "        lambda w: delete_letter(w)[0],\n",
        "        lambda w: replace_letter(w)[0],\n",
        "        lambda w: insert_letter(w)[0],\n",
        "        lambda w: transpose_letters(w)[0],\n",
        "    ])(word)\n",
        "\n",
        "# Створення тестової вибірки\n",
        "test_sample = [w for w in set(shakespeare_words) if w in word_freq][:100]\n",
        "typo_pairs = [(w, make_typos(w)) for w in test_sample]\n",
        "\n",
        "# Оцінка\n",
        "def evaluate(pairs):\n",
        "    correct = 0\n",
        "    for correct_word, typo in pairs:\n",
        "        predicted = correction(typo)\n",
        "        if predicted == correct_word:\n",
        "            correct += 1\n",
        "    return correct / len(pairs)\n",
        "\n",
        "accuracy = evaluate(typo_pairs)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR7wN4ILZ3ti",
        "outputId": "bfd5cdf7-956d-4dfa-eaaa-825dc3bfbb76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Кусок тексту для тесту замість файлу\n",
        "sample_text = \"\"\"\n",
        "To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "\"\"\"\n",
        "\n",
        "# Токенізація\n",
        "sample_words = tokenize(sample_text)\n",
        "\n",
        "# Вибірка слів для тесту: ті, що є у словнику частот\n",
        "test_sample = [w for w in set(sample_words) if w in word_freq]\n",
        "\n",
        "# Створення пар (правильне слово, слово з помилкою)\n",
        "typo_pairs = [(w, make_typos(w)) for w in test_sample]\n",
        "\n",
        "# Оцінка точності\n",
        "accuracy = evaluate(typo_pairs)\n",
        "print(\"Accuracy on sample text:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO9bJMuvajKb",
        "outputId": "f5ecfe85-d005-4b0c-d135-5dcb7de80faa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on sample text: 0.9545454545454546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Збереження словника частот та total_words у JSON\n",
        "model_data_serializable = {\n",
        "    \"word_freq\": dict(word_freq),\n",
        "    \"total_words\": total_words\n",
        "}\n",
        "\n",
        "with open(\"word_freq_model.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(model_data_serializable, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Model saved to word_freq_model.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClnrqS1Zd8U-",
        "outputId": "75f36bf5-1a2f-4a3b-9610-25835d345c4e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to word_freq_model.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "def load_model(filename=\"word_freq_model.json\"):\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return Counter(data[\"word_freq\"]), data[\"total_words\"]\n",
        "\n",
        "# Приклад завантаження:\n",
        "# loaded_word_freq, loaded_total_words = load_model()\n",
        "# print(\"Loaded words count:\", len(loaded_word_freq))\n"
      ],
      "metadata": {
        "id": "K0sgU1czeAlU"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}